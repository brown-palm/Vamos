<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Vamos: Versatile Action Models for Video Understanding">
  <meta property="og:title" content="Vamos: Versatile Action Models for Video Understanding"/>
  <meta property="og:description" content=""/>
  <meta property="og:url" content="https://brown-palm.github.io/Vamos"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="https://brown-palm.github.io/Vamos/static/images/model_vamos.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Vamos: Versatile Action Models for Video Understanding">
  <meta name="twitter:description" content="">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="https://brown-palm.github.io/Vamos/static/images/model_vamos.png">
  <meta name="twitter:card" content="">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Video Understanding, Large Language Model, Video Question Answering, Long-term Action Anticipation">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Vamos: Versatile Action Models for Video Understanding</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Vamos: Versatile Action Models for Video Understanding</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://wang-sj16.github.io/" target="_blank">Shijie Wang</a>,</span>
                <span class="author-block">
                  <a href="" target="_blank">Qi Zhao</a>,</span>
                  <span class="author-block">
                    <a href="" target="_blank">Minh Quan Do</a>,</span>
                    <span class="author-block">
                      <a href="" target="_blank">Nakul Agarwal</a>,</span>
                      <span class="author-block">
                        <a href="" target="_blank">Kwonjoon Lee</a>,</span>
                    <span class="author-block">
                        <a href="https://chensun.me/" target="_blank">Chen Sun</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Brown University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
                    <span class="author-block">Honda Research Institute</span>

                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">

                  <!-- ArXiv abstract Link -->
                <span class="link-block">
                    <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
    
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (coming soon)</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            What makes good video representations for video understanding, such as anticipating future activities, or answering video-conditioned questions? While earlier approaches focus on end-to-end learning directly from video pixels, we propose to revisit text-based representations, such as discrete action labels, or free-form video captions, which are interpretable and can be directly consumed by large language models (LLMs). Intuitively, different video understanding tasks may require representations that are complementary and at different granularities. To this end, we propose versatile action models (Vamos), a learning framework powered by a large language model as the ``reasoner'', and can flexibly leverage visual embeddings, action labels, and free-form descriptions extracted from videos as its input. We evaluate Vamos on four complementary video understanding benchmarks, Ego4D, Next-QA, IntentQA, and EgoSchema, on its capability to model temporal dynamics, encode visual history, and perform reasoning. Surprisingly, we observe that text-based representations consistently achieve competitive performance on all benchmarks, and that visual embeddings provide marginal or no performance improvement, demonstrating the effectiveness of text-based video representation in the LLM era. We perform extensive ablation study and qualitative analysis to support our observations, and achieve state-of-the-art performance on three benchmarks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Method: Vamos-->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="content">
      <h2 class="title">Vamos: Versatile Action Models</h2>
      <p>We introduce Vamos, a simple yet effective framework to utilize LLMs to unify video dynamic modeling tasks, including comprehending historical content (video question answering, VQA) and future prediction (long-term action anticipation, LTA). Vamos flexibly unifies distributed visual features and textual video representations including discrete action labels and free-form video captions.</p>
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img src="static/images/model_vamos.png" alt="Vamos Model" height="100%"/>
        </div>
      </div>
      </div>
    </div>
  </div>
</section>
<!--End Method -->


<!-- Method: Token selector-->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="content">
      <h2 class="title">Lightweight Token Selector</h2>
      <p>To investigate the potential of compressing long input sequences and extracting keys element from the generated textual representations to probe the crucial information for downstream video understanding tasks, we introduce a lightweight token selector as an add-on module of Vamos. It pick one single token from the video sequence based on the task sequence. By applying the token selector to k segments of textual video representation, k tokens are finally selected as the compact input to the LLM for down-stream tasks. </p>
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img src="static/images/model_selector.png" alt="Vamos selector" height="100%"/>
        </div>
      </div>
      </div>
    </div>
  </div>
</section>
<!--End Method -->

<!-- Method: Vamos-->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title">Visualization of Vamos Prediction and Manual Intervention</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
         <!-- Your image here -->
          <img src="static/images/vis.png" alt="antmaze_score"/>
         <h2 class="subtitle has-text-centered">
          Vamos predictions and manual intervention.
         </h2>
       </div>
       <div class="item">
         <!-- Your image here -->
         <img src="static/images/vis_token.png" alt="kitchen_score"/>
         <h2 class="subtitle has-text-centered">
          Predictions with token selector (to 40 tokens) and manual intervention.
         </h2>
       </div>
   </div>
    </div>
  </div>
</section>
<!--End Method -->


  <!-- Effectiveness-->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title">Benchmark Results</h2>
      <p>We compare Vamos with other state-of-the-art models on the four benchmarks: EgoSchema, NeXT-QA, IntentQA, and Ego4D LTA. </p>

      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
         <!-- Your image here -->
          <img src="static/images/egoschema.png" alt="antmaze_score"/>
         <h2 class="subtitle has-text-centered">
          Egoschema VQA zero-shot Performance.
         </h2>
       </div>
       <div class="item">
         <!-- Your image here -->
         <img src="static/images/nextqa.png" alt="kitchen_score"/>
         <h2 class="subtitle has-text-centered">
          NeXT-QA benchmark. * denotes model using additional dataset during training.
         </h2>
       </div>
       <div class="item">
         <!-- Your image here -->
         <img src="static/images/intentqa.png" alt="gym_score"/>
         <h2 class="subtitle has-text-centered">
          IntentQA benchmark.
        </h2>
      </div>
      <div class="item">
       <!-- Your image here -->
       <img src="static/images/ego4d_lta.png" alt="antmaze_rliable"/>
       <h2 class="subtitle has-text-centered">
        Ego4D LTA v2 test set.
       </h2>
     </div>
   </div>
    </div>
  </div>
</section>
  <!-- End Effectiveness-->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title">Acknowledgements</h2>
      <p>
        This work is supported by Honda Research Institute USA and Samsung Advanced Institute of Technology. We would like to thank Apoorv Khandelwal, Calvin Luo, David Isele, Songpo Li, and Tian Yun for their generous feedback on this work. Our research was conducted using computational resources at the Center for Computation and Visualization, Brown University.
      </p>
</div>
</div>
</section>
<!--End Effectiveness -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
  </footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
</html>